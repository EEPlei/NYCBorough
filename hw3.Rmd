---
title: Homework 3
author: Duke Dinosaurs Team 4
date: "October 24, 2015"
output: html_document
---
##Geocoding##
First obtain the data using the following code `fread() %>% tbl_df()`
Since we have such a large dataset, we begin by learning some things about our data. Then, we can begin cleaning the data, so we have accurate data to work with. 
Let's take a look at what kind of information our data tells us.     
`colnames(nyc)`
We'll get back the folowing results       
 ` "Unique.Key"                     "Created.Date"`                  
 ` "Closed.Date"                    "Agency"`                        
 ` "Agency.Name"                    "Complaint.Type"`                
 ` "Descriptor"                     "Location.Type"`                 
 ` "Incident.Zip"                   "Incident.Address"`              
 ` "Street.Name"                    "Cross.Street.1"`                
 ` "Cross.Street.2"                 "Intersection.Street.1"`         
 ` "Intersection.Street.2"          "Address.Type"`                  
 ` "City"                           "Landmark"`                      
 ` "Facility.Type"                  "Status"`                        
 ` "Due.Date"                       "Resolution.Description"`        
 ` "Resolution.Action.Updated.Date" "Community.Board"`               
 ` "Borough"                        "Park.Facility.Name"`            
 ` "Park.Borough"                   "School.Name"`                   
 ` "School.Number"                  "School.Region"`                 
 ` "School.Code"                    "School.Phone.Number"`           
 ` "School.Address"                 "School.City"`                   
 ` "School.State"                   "School.Zip"`                    
 ` "School.Not.Found"               "School.or.Citywide.Complaint"`  
 ` "Vehicle.Type"                   "Taxi.Company.Borough"`          
 ` "Taxi.Pick.Up.Location"          "Bridge.Highway.Name"`           
 ` "Bridge.Highway.Direction"       "Road.Ramp"`                    
 ` "Bridge.Highway.Segment"         "Garage.Lot.Name"`               
 ` "Ferry.Direction"                "Ferry.Terminal.Name"`   
Since some of these aren't important, we'll subset our `nyc` data to just the following fourteen variables.
                        `"Unique.Key",`         
                        `"Complaint.Type",`          
                        `"Descriptor",`            
                        `"Incident.Zip",`           
                        `"Incident.Address",`            
                        `"Street.Name",`              
                        `"Cross.Street.1",`                   
                        `"Cross.Street.2",`                     
                        `"Intersection.Street.1",`                  
                        `"Intersection.Street.2",`                          
                        `"Address.Type",`                
                        `"City",`                      
                        `"Resolution.Description",`
                        `"Borough"`               
            
We can see that location information is available through two Cross Street columns or two Intersection Street columns. However, we shouldn't assume that all data points will have information in these columns. We should test to see if there exists cases where there are no information.    
`test1 <- nyc.important$Intersection.Street.1 != ""`    
`test2 <- nyc.important$Intersection.Street.2 != ""`    
`> length(which(test1))`     
`[1] 1817002`     
`> length(which(test2))`     
`[1] 1816062`     
So we can see that there may exist cases where Intersection.Street.1 has data but Intersetion.Street.2 does not. This may also be true vice-versa.    
The data that we want should either have both Cross Street data or both Intersection Street data. 
`nyc1` will filter out all nyc data where Cross.Street.1 is not empty and Cross.Street.2 is not empty. Since Intersection.Street.1 or Intersection.Street may also be full at the same time, we'll filter `nyc1` into `nyc1.1` by only taking the data points where both Intersection columns are empty.    
Similarily, `nyc2` will find all nyc data where Intersection.Street.1 is not empty and Intersection.Street.2 is not empty. `nyc2.1` will further filter the data by finding the ones where both Cross columns are empty. 

Find all the data where both Intersection Street Columns are filled.    
Now we have two subsets of our `nyc.important` data frame. We've thrown away the data where there isn't enough information. Now, we consider the possibility that there may be too much information. In this case, it would be the data where there is information for both Cross Street Columns as well as (both Intersection Street Columns. We can bind `nyc1` and `nyc2` together and throw away the data points where we have both cross and intersection information.    
`nyc3` binds `nyc1.1` and `nyc2.1`. Now we have a data frame where either both Cross Columns are full and both Intersection Columns are empty or both Intersection Columns are full and both Cross Columns are empty.    
`nyc4` makes sure that all data points has location information 
Let's now look at the Borough Column.       
`> unique(nyc4$Borough)`
`"MANHATTAN"`
`"BRONX"`         
`"BROOKLYN"`      
`"QUEENS"`        
`"STATEN ISLAND"` 
`"Unspecified"`
Since we are only interested in the first five boroughs, `nyc5` will filter out "Unspecified".    
Find all city names that has "new" in any combination of upper and lower case. Find all city names that has some variation of "york" in it. Note, we know that there is the variation "yok" because we saw it when looking at `sort(unique(nyc$City))`. It is entirely possible that we missed a variation of New York, but this is our best attempt.
Now we have a vector of names we can assume was meant to be New York City. We can filter out all data points whose City Column entry is not one of those names - `nyc6`.
By now, we've realized that each data's location data is given to us differently. Looking at the Address.Type column, we can have some idea how the data is presented to us.     
`> unique(nyc$Address.Type)`    
`[1] ""  "ADDRESS" "INTERSECTION" "LATLONG" "BLOCKFACE" "PLACENAME"`     
We only want to work with ""Address", "Intersection", and "Placename". `nyc7` and `nyc7.1` will remove all data whose Address.Type Column isn't that of the three above. 

##Visualization##
After data cleaning, our final data consists of 628,250 observations with the corresponding longitude, latitude, and borough of the reported incident. First and foremost, we wanted to make sure that wercker would recognize our column variables `long` and `lat`, which, respectively, stand for `longitude` and `latitude`. 
`names(data) = c("Borough","long","lat")`    

We then took a random sample of the data. `data1 <- sample_frac(data, 0.1)` 
We conducted this random sampling based on two reasons. One, we believed that more data did not necessarily translate into better results as the location of the incident is also a random variable. That is, the location of the incident, may be within a borough or may be located on a borderline between two boroughs. This reasoning leads us to our second reasoning, where less data could exponentially reduce the running time of the svm model we utilized. Upon various levels of sensitivity analysis, we concluded not even though it may take 10 minutes to analyze 5% of the data, the increment in time does not necessarily vary linearly with the increment in percentage. That is, although it may only take 10 minutes to predict and plot 5% of the data, that does not mean that it will take 100 minutes to analyze 50% of the data. Thus, we believed that, in order to optimize performance and efficiency, we needed to take a random sample of the cleaned data. Via sensitivity analysis, we concluded that a 10% sample of the total data, performed the best and was the most efficient. 

Also, we decided that generating "fake data" was crucial in the performance of our svm model. This was based on the reasoning that fake data waas needed to help svm differentiate between points that are actually in one of the boroughs in New York and points that either, 1) belonged to the various bodies of water surrounding New York City or 2) belonged to non-New York regions, such as New Jersey. Thus, through pulling off coordinates from the GoogleMap and through usage of the `runif` command to generate larger areas with non-NYC coordinates, we were able to generate approximately 48000 non-NYC coordinates. We then combined the 10% of the cleaned data with all of the non-NYC data to build `data2`. 
`data2 <- rbind(data1,sample)` 

We converted the `long` and `lat` in `data2` to a numeric variable and necessarily dropped any NAs that came up via coercion. Finally, for svm to run, we converted `data2$Borough` to be a factor variable with six levels--Bronx, Brooklyn, Manhattan, Queens, Staten Island, and notNYC. 

We implemented a svm model `l<-svm(Borough ~ ., data2, kernel = "radial")`. In constructing the raster rectangle, we concluded that the automation of the contruction was necessary as we our data was a 10% sample of the original cleaned data. That is, our coordinates can differ according to the random sample r takes during each code run. Hence, we first specified a `dims` variable--namely, dimension--which takes the square root of the number of rows in `data2`, adds one to it, and throws out the decimals. Then we also specified a `boundary` variable which gives us the maximum and minimum of the `long` and `lat` variables in `data2`. As a final step, we set up the raster rectangle. We then proceed to predicting and plotting the points on our raster rectangle.

