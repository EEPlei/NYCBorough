---
title: Homework 3
author: Duke Dinosaurs Team 4
date: "October 24, 2015"
output: html_document
---


##Geocoding##
First obtain the data using the SQLite package since we have very large data, it's faster to read than fread function. Our code is as follows.
```
base_path <- "~cr173/Sta523/data/nyc/"
database <- dbConnect(drv=RSQLite::SQLite(), 
                      dbname="~cr173/Sta523/data/nyc/nyc_311_index.sqlite")
tables <- dbListTables(database)
lDataFrames <- vector("list", length=length(tables))

## create a data.frame for each table
for (i in seq(along=tables)) {
  lDataFrames[[i]] <- dbGetQuery(conn=database, 
                                 statement=paste("SELECT * FROM '", 
                                                 tables[[i]], "'", sep=""))
}
```
Since we have such a large dataset, we begin by learning some things about our data. Then, we can begin cleaning the data, so we have accurate data to work with. 
Let's take a look at what kind of information our data tells us.     
`colnames(nyc)`
We'll get back the folowing results       
 ` "Unique.Key"                     "Created.Date"`                  
 ` "Closed.Date"                    "Agency"`                        
 ` "Agency.Name"                    "Complaint.Type"`                
 ` "Descriptor"                     "Location.Type"`                 
 ` "Incident.Zip"                   "Incident.Address"`              
 ` "Street.Name"                    "Cross.Street.1"`                
 ` "Cross.Street.2"                 "Intersection.Street.1"`         
 ` "Intersection.Street.2"          "Address.Type"`                  
 ` "City"                           "Landmark"`                      
 ` "Facility.Type"                  "Status"`                        
 ` "Due.Date"                       "Resolution.Description"`        
 ` "Resolution.Action.Updated.Date" "Community.Board"`               
 ` "Borough"                        "Park.Facility.Name"`            
 ` "Park.Borough"                   "School.Name"`                   
 ` "School.Number"                  "School.Region"`                 
 ` "School.Code"                    "School.Phone.Number"`           
 ` "School.Address"                 "School.City"`                   
 ` "School.State"                   "School.Zip"`                    
 ` "School.Not.Found"               "School.or.Citywide.Complaint"`  
 ` "Vehicle.Type"                   "Taxi.Company.Borough"`          
 ` "Taxi.Pick.Up.Location"          "Bridge.Highway.Name"`           
 ` "Bridge.Highway.Direction"       "Road.Ramp"`                    
 ` "Bridge.Highway.Segment"         "Garage.Lot.Name"`               
 ` "Ferry.Direction"                "Ferry.Terminal.Name"`   
Since some of these aren't important, we'll subset our `nyc` data to just the following fourteen variables.
                        `"Unique.Key",`         
                        `"Complaint.Type",`          
                        `"Descriptor",`            
                        `"Incident.Zip",`           
                        `"Incident.Address",`            
                        `"Street.Name",`              
                        `"Cross.Street.1",`                   
                        `"Cross.Street.2",`                     
                        `"Intersection.Street.1",`                  
                        `"Intersection.Street.2",`                          
                        `"Address.Type",`                
                        `"City",`                      
                        `"Resolution.Description",`
                        `"Borough"`               
            
We can see that location information is available through two Cross Street columns or two Intersection Street columns. However, we shouldn't assume that all data points will have information in these columns. We should test to see if there exists cases where there are no information.    
`test1 <- nyc.important$Intersection.Street.1 != ""`    
`test2 <- nyc.important$Intersection.Street.2 != ""`    
`> length(which(test1))`     
`[1] 1817002`     
`> length(which(test2))`     
`[1] 1816062`     
So we can see that there may exist cases where Intersection.Street.1 has data but Intersetion.Street.2 does not. This may also be true vice-versa.    
The data that we want should either have both Cross Street data or both Intersection Street data. 
`nyc1` will filter out all nyc data where Cross.Street.1 is not empty and Cross.Street.2 is not empty. Since Intersection.Street.1 or Intersection.Street may also be full at the same time, we'll filter `nyc1` into `nyc1.1` by only taking the data points where both Intersection columns are empty.    
Similarily, `nyc2` will find all nyc data where Intersection.Street.1 is not empty and Intersection.Street.2 is not empty. `nyc2.1` will further filter the data by finding the ones where both Cross columns are empty. 

Find all the data where both Intersection Street Columns are filled.    
Now we have two subsets of our `nyc.important` data frame. We've thrown away the data where there isn't enough information. Now, we consider the possibility that there may be too much information. In this case, it would be the data where there is information for both Cross Street Columns as well as (both Intersection Street Columns. We can bind `nyc1` and `nyc2` together and throw away the data points where we have both cross and intersection information.    
`nyc3` binds `nyc1.1` and `nyc2.1`. Now we have a data frame where either both Cross Columns are full and both Intersection Columns are empty or both Intersection Columns are full and both Cross Columns are empty.    
`nyc4` makes sure that all data points has location information 
Let's now look at the Borough Column.       
`> unique(nyc4$Borough)`
`"MANHATTAN"`
`"BRONX"`         
`"BROOKLYN"`      
`"QUEENS"`        
`"STATEN ISLAND"` 
`"Unspecified"`
Since we are only interested in the first five boroughs, `nyc5` will filter out "Unspecified".    
Find all city names that has "new" in any combination of upper and lower case. Find all city names that has some variation of "york" in it. Note, we know that there is the variation "yok" because we saw it when looking at `sort(unique(nyc$City))`. It is entirely possible that we missed a variation of New York, but this is our best attempt.
Now we have a vector of names we can assume was meant to be New York City. We can filter out all data points whose City Column entry is not one of those names - `nyc6`.
By now, we've realized that each data's location data is given to us differently. Looking at the Address.Type column, we can have some idea how the data is presented to us.     
`> unique(nyc$Address.Type)`    
`[1] ""  "ADDRESS" "INTERSECTION" "LATLONG" "BLOCKFACE" "PLACENAME"`     
We only want to work with ""Address", "Intersection", and "Placename". `nyc7` and `nyc7.1` will remove all data whose Address.Type Column isn't that of the three above. Our final cleaned version of nyc data is `nyc7.2`.

Now we have cleaned version of nyc data, then we load intersections data from the path `~cr173/Sta523/data/nyc/intersections/`. We read intersections shapefile through readOGR function and then selected the intersections data we want. There is no coordinated in intersections at the beginning. Then we used coordinates function to get longtitude and latitude for each record and combined it to the intersections data. We selected     "Streets","Longitude","Latitude","Street1","Street2" columns from it as inter_data which could be used for combining with cleaned nyc data.

Then we extracted intersections data from `nyc7.2`. We found there are two kinds of intersections columns which are Cross.Street.1/Cross.Street.2 and Intersection.Street.1/Interseciton.Street.2. When there is value in Cross.Street.1/Cross.Street.2 there would be no value in      Intersection.Street.1/Interseciton.Street.2 and vice versa. So we selected both Cross.Street.1/Cross.Street.2 columns, Intersection.Street.1/Interseciton.Street.2 columns and also borough. Then we have nyc_inter1 which contained Cross.Street.1, Cross.Street.2 and borough, nyc_inter2 which contained Intersection.Street.1, Intersection.Street.2 and borough. Combined nyc_inter1 with nyc_inter2 as nyc_inter which contained Street1, Street2 and borough columns. Then we used unique filter the nyc_inter.     

Next step we worked on merging nyc_inter with inter_data. First we should clean both of them to make them in the same format. We found in nyc_inter data most of values were in full name while most of values were in short name in inter_data. So we need to change that. We created `transform` function which changed all full names like AVENUE, STREET, DRIVE, BOULEVARD and so on to short names accordingly. `transform` function is like this.
```
transform = function(v){
  v = str_replace_all(str_c(v), 
                      c("AVENUE"="AVE", "STREET"="ST", "DRIVE"="DR", 
                        "BOULEVARD"="BLVD", "ROAD"="RD", "PLACE"="PL", 
                        "EXPRESSWAY"="EXPY", "PARKWAY"="PKWY", 
                        "FREEWAY"="FWY", "CRESCENT"="CRES", "COURT"="CT",
                        "LANE"="LN", "EAST"="E", "WEST"="W", "SOUTH "="S",
                        "NORTH"="N"))
}
```
Then we applied transform function to both nyc_inter and inter_data. Next we merged nyc_inter and inter_data as res1 by matching Street1 and Street2 columns. Also we switched Street1 and Street2 columns to match again as res2. Combined res1 with res2 together as our final merged result, res_inter.     

Next we performed the similer process for pluto data. We loaded pluto data from `/home/vis/cr173/Sta523/data/nyc/pluto/pluto.Rdata`. And then we filterd `nyc7.2` by Address.Type == "ADDRESS" and extrated Incident.zip, Incident.address and borough columns from it as nyc_pluto. Then we merged nyc_pluto with pluto by matching "Address". Now we have two products, res_pluto and res_inter. We combined them together as "data" to plot later.     
   
Our method for cleaning the data isn't perfect. A lot of information was entered incorrectly. When I attempted to clean it by only sorting out data points where the `City` column contains variations of New York City, I was able to clean out of a lot of data, but I ended up with only three boroughs instead of five. It might have been possible that only three boroughs had calls outgoing to the call center, but as shown in `boroughs.json` calls originated from all five boroughs. Not only that, but there are numerous locations where numerous calls will originate from. It is also difficult to distinguish between interior data points and exterior data points that lie on the boundary of each borough. In the next section, you can see that it will be necessary to generate Not New York City data points to make the boundaries of New York City clearer.     

##Borough##
After data cleaning, our final data consists of 628,250 observations with the corresponding longitude, latitude, and borough of the reported incident. First and foremost, we wanted to make sure that wercker would recognize our column variables `long` and `lat`, which, respectively, stand for `longitude` and `latitude`. 
`names(data) = c("Borough","long","lat")`    

We then took a random sample of the data. `data1 <- sample_frac(data, 0.1)` 
We conducted this random sampling based on two reasons. One, we believed that more data did not necessarily translate into better results as the location of the incident is also a random variable. That is, the location of the incident, may be within a borough or may be located on a borderline between two boroughs. This reasoning leads us to our second reasoning, where less data could exponentially reduce the running time of the svm model we utilized. Upon various levels of sensitivity analysis, we concluded not even though it may take 10 minutes to analyze 5% of the data, the increment in time does not necessarily vary linearly with the increment in percentage. That is, although it may only take 10 minutes to predict and plot 5% of the data, that does not mean that it will take 100 minutes to analyze 50% of the data. Thus, we believed that, in order to optimize performance and efficiency, we needed to take a random sample of the cleaned data. Via sensitivity analysis, we concluded that a 10% sample of the total data, performed the best and was the most efficient. 

Also, we decided that generating "fake data" was crucial in the performance of our svm model. This was based on the reasoning that fake data waas needed to help svm differentiate between points that are actually in one of the boroughs in New York and points that either, 1) belonged to the various bodies of water surrounding New York City or 2) belonged to non-New York regions, such as New Jersey. Our function `notNYC` first randomly samples a pre-specified number N from the raster rectangle with `(xmin, xmax) = (-74.3, -73.67)` and `(ymin, ymax) = (40.5, 40.91)`. Through sensitivity analysis, we concluded that N 20,000 achieved the best performing output in the shortest time. Once the 20,000 points are randomly sampled from the raster rectangle, we added and subtracted `md200 <- 200 * 1/111111` which is equivalent to 200 meters, in longitude and latitude units. Thus, through the `notNYC` function, we were able to label points that were more than 200 meters away from the cleaned data `N` (thus not-NYC) and label all points that were within 200 meters from the cleaned data `Y` (thus in NYC). Thus, the `notNYC` function created a boundary of points that surrounded the outer most points of the five boroughs. We also devised the `notNYC` function so that it includes the points in the Central Park as `Y`. This was done by manually collecting the coordinates of the four vertices. We found out that the respective coordinates were as follows.    
`Right-most bottom corner: (40.764382, -73.973024)`,        
`Left-most bottom corner: (40.768152, -73.981464)`,         
`Right-most top corner: (40.796844, -73.949389)`,         
`Left-most top corner: (40.800716, -73.958233)`         

With the four vertices specified, we then designed an algorithm that specified a point--if it fell within the range of the central park--would be labeled `Y`. Through `notNYC` we generated quantity `N` amount of points that were not-NYC and merged with the sample of the cleaned data with all of the non-NYC data to build `data2`. 
`data2 <- rbind(data1,Not_NYC1)` 

We converted the `long` and `lat` in `data2` to a numeric variable and necessarily dropped any NAs that came up via coercion. Finally, for svm to run, we converted `data2$Borough` to be a factor variable with six levels--Bronx, Brooklyn, Manhattan, Queens, Staten Island, and notNYC. 

We implemented a svm model `l<-svm(Borough ~ ., data2, kernel = "radial")`. In constructing the raster rectangle, we concluded that the automation of the contruction was necessary as we our data was a 10% sample of the original cleaned data. That is, our coordinates can differ according to the random sample r takes during each code run. Hence, we first specified a `dims` variable--namely, dimension--which takes the square root of the number of rows in `data2`, adds one to it, and throws out the decimals. Then we also specified a `boundary` variable which gives us the maximum and minimum of the `long` and `lat` variables in `data2`. As a final step, we set up the raster rectangle. We then proceed to predicting and plotting the points on our raster rectangle.


##Visual##   

For the visual task, we chose to show how effective the call center was at handling incoming calls. For our cleaned data, there is a column called `Status` that will tell you whether a complaint is Pending, Closed, Assigned, Open, or Unassigned. Since `Status` was unimportant while we were creating `boroughs.json`, we excluded `Status` then. Now, we utilize the same method we used earlier with `Status` to merge our cleaned data with our intersection data. 

If we run `nrow` on each `stat_inter` we will be able to see the following. 
`> nrow(stat_inter1)`     
`[1] 30764`      
`> nrow(stat_inter2)`      
`[1] 76826`       
`> nrow(stat_inter3)`     
`[1] 8940`       
`> nrow(stat_inter4)`       
`[1] 13554`      
`> nrow(stat_inter5)`      
`[1] 1262`      
`> nrow(stat_inter6)`      
`[1] 2`       
From the plot we can see that the number of Closed calls dominates. The next most frequent is Pending calls. Open calls and Assigned calls are the next most frequent. Followed by Started and Unassigned. Unassigned has only two points and are essentially invisible on our plot. When plotting, we can see that after plotting Pending calls, if we add the points for Closed calls, the blue Closed calls points will completely overwhelm the red Pending calls points. However, when we add the ponts for Open, Assigned, and Unassigned, we can see them visibly appear on the map. This tells us that the majority of Closed calls are located at places where Pending calls are also located. Since, the following calls are visible, we can see that there are some locations in New York, where calls will frequently occur. These calls are easily handled as evidenced by the Closed and Pending locations. Lastly, aside from the two Unassigned points, we can say that these calls are pretty evenly distributed throughout New York City.    