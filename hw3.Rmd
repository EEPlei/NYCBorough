---
title: Homework 3
author: Duke Dinosaurs Team 4
date: "October 24, 2015"
output: html_document
---
##Geocoding##
First obtain the data using the SQLite package since we have very large data, it's faster to read than fread function. Our code is as follows.
```
base_path <- "~cr173/Sta523/data/nyc/"
database <- dbConnect(drv=RSQLite::SQLite(), 
                      dbname="~cr173/Sta523/data/nyc/nyc_311_index.sqlite")
tables <- dbListTables(database)
lDataFrames <- vector("list", length=length(tables))

## create a data.frame for each table
for (i in seq(along=tables)) {
  lDataFrames[[i]] <- dbGetQuery(conn=database, 
                                 statement=paste("SELECT * FROM '", 
                                                 tables[[i]], "'", sep=""))
}
```
Since we have such a large dataset, we begin by learning some things about our data. Then, we can begin cleaning the data, so we have accurate data to work with. 
Let's take a look at what kind of information our data tells us.     
`colnames(nyc)`
We'll get back the folowing results       
 ` "Unique.Key"                     "Created.Date"`                  
 ` "Closed.Date"                    "Agency"`                        
 ` "Agency.Name"                    "Complaint.Type"`                
 ` "Descriptor"                     "Location.Type"`                 
 ` "Incident.Zip"                   "Incident.Address"`              
 ` "Street.Name"                    "Cross.Street.1"`                
 ` "Cross.Street.2"                 "Intersection.Street.1"`         
 ` "Intersection.Street.2"          "Address.Type"`                  
 ` "City"                           "Landmark"`                      
 ` "Facility.Type"                  "Status"`                        
 ` "Due.Date"                       "Resolution.Description"`        
 ` "Resolution.Action.Updated.Date" "Community.Board"`               
 ` "Borough"                        "Park.Facility.Name"`            
 ` "Park.Borough"                   "School.Name"`                   
 ` "School.Number"                  "School.Region"`                 
 ` "School.Code"                    "School.Phone.Number"`           
 ` "School.Address"                 "School.City"`                   
 ` "School.State"                   "School.Zip"`                    
 ` "School.Not.Found"               "School.or.Citywide.Complaint"`  
 ` "Vehicle.Type"                   "Taxi.Company.Borough"`          
 ` "Taxi.Pick.Up.Location"          "Bridge.Highway.Name"`           
 ` "Bridge.Highway.Direction"       "Road.Ramp"`                    
 ` "Bridge.Highway.Segment"         "Garage.Lot.Name"`               
 ` "Ferry.Direction"                "Ferry.Terminal.Name"`   
Since some of these aren't important, we'll subset our `nyc` data to just the following fourteen variables.
                        `"Unique.Key",`         
                        `"Complaint.Type",`          
                        `"Descriptor",`            
                        `"Incident.Zip",`           
                        `"Incident.Address",`            
                        `"Street.Name",`              
                        `"Cross.Street.1",`                   
                        `"Cross.Street.2",`                     
                        `"Intersection.Street.1",`                  
                        `"Intersection.Street.2",`                          
                        `"Address.Type",`                
                        `"City",`                      
                        `"Resolution.Description",`
                        `"Borough"`               
            
We can see that location information is available through two Cross Street columns or two Intersection Street columns. However, we shouldn't assume that all data points will have information in these columns. We should test to see if there exists cases where there are no information.    
`test1 <- nyc.important$Intersection.Street.1 != ""`    
`test2 <- nyc.important$Intersection.Street.2 != ""`    
`> length(which(test1))`     
`[1] 1817002`     
`> length(which(test2))`     
`[1] 1816062`     
So we can see that there may exist cases where Intersection.Street.1 has data but Intersetion.Street.2 does not. This may also be true vice-versa.    
The data that we want should either have both Cross Street data or both Intersection Street data. 
`nyc1` will filter out all nyc data where Cross.Street.1 is not empty and Cross.Street.2 is not empty. Since Intersection.Street.1 or Intersection.Street may also be full at the same time, we'll filter `nyc1` into `nyc1.1` by only taking the data points where both Intersection columns are empty.    
Similarily, `nyc2` will find all nyc data where Intersection.Street.1 is not empty and Intersection.Street.2 is not empty. `nyc2.1` will further filter the data by finding the ones where both Cross columns are empty. 

Find all the data where both Intersection Street Columns are filled.    
Now we have two subsets of our `nyc.important` data frame. We've thrown away the data where there isn't enough information. Now, we consider the possibility that there may be too much information. In this case, it would be the data where there is information for both Cross Street Columns as well as (both Intersection Street Columns. We can bind `nyc1` and `nyc2` together and throw away the data points where we have both cross and intersection information.    
`nyc3` binds `nyc1.1` and `nyc2.1`. Now we have a data frame where either both Cross Columns are full and both Intersection Columns are empty or both Intersection Columns are full and both Cross Columns are empty.    
`nyc4` makes sure that all data points has location information 
Let's now look at the Borough Column.       
`> unique(nyc4$Borough)`
`"MANHATTAN"`
`"BRONX"`         
`"BROOKLYN"`      
`"QUEENS"`        
`"STATEN ISLAND"` 
`"Unspecified"`
Since we are only interested in the first five boroughs, `nyc5` will filter out "Unspecified".    
Find all city names that has "new" in any combination of upper and lower case. Find all city names that has some variation of "york" in it. Note, we know that there is the variation "yok" because we saw it when looking at `sort(unique(nyc$City))`. It is entirely possible that we missed a variation of New York, but this is our best attempt.
Now we have a vector of names we can assume was meant to be New York City. We can filter out all data points whose City Column entry is not one of those names - `nyc6`.
By now, we've realized that each data's location data is given to us differently. Looking at the Address.Type column, we can have some idea how the data is presented to us.     
`> unique(nyc$Address.Type)`    
`[1] ""  "ADDRESS" "INTERSECTION" "LATLONG" "BLOCKFACE" "PLACENAME"`     
We only want to work with ""Address", "Intersection", and "Placename". `nyc7` and `nyc7.1` will remove all data whose Address.Type Column isn't that of the three above. Our final cleaned version of nyc data is `nyc7.2`.

Now we have cleaned version of nyc data, then we load intersections data from the path `~cr173/Sta523/data/nyc/intersections/`. We read intersections shapefile through readOGR function and then selected the intersections data we want. There is no coordinated in intersections at the beginning. Then we used coordinates function to get longtitude and latitude for each record and combined it to the intersections data. We selected "Streets","Longitude","Latitude","Street1","Street2" columns from it as inter_data which could be used for combining with cleaned nyc data.

Then we extracted intersections data from `nyc7.2`. We found there are two kinds of intersections columns which are Cross.Street.1/Cross.Street.2 and Intersection.Street.1/Interseciton.Street.2. When there is value in Cross.Street.1/Cross.Street.2 there would be no value in Intersection.Street.1/Interseciton.Street.2 and vice versa. So we selected both Cross.Street.1/Cross.Street.2 columns, Intersection.Street.1/Interseciton.Street.2 columns and also borough. Then we have nyc_inter1 which contained Cross.Street.1, Cross.Street.2 and borough, nyc_inter2 which contained Intersection.Street.1, Intersection.Street.2 and borough. Combined nyc_inter1 with nyc_inter2 as nyc_inter which contained Street1, Street2 and borough columns. Then we used unique filter the nyc_inter.

Next step we worked on merging nyc_inter with inter_data. First we should clean both of them to make them in the same format. We found in nyc_inter data most of values were in full name while most of values were in short name in inter_data. So we need to change that. We created `transform` function which changed all full names like AVENUE, STREET, DRIVE, BOULEVARD and so on to short names accordingly. `transform` function is like this.
```
transform = function(v){
  v = str_replace_all(str_c(v), 
                      c("AVENUE"="AVE", "STREET"="ST", "DRIVE"="DR", 
                        "BOULEVARD"="BLVD", "ROAD"="RD", "PLACE"="PL", 
                        "EXPRESSWAY"="EXPY", "PARKWAY"="PKWY", 
                        "FREEWAY"="FWY", "CRESCENT"="CRES", "COURT"="CT",
                        "LANE"="LN", "EAST"="E", "WEST"="W", "SOUTH "="S",
                        "NORTH"="N"))
}
```
Then we applied transform function to both nyc_inter and inter_data. Next we merged nyc_inter and inter_data as res1 by matching Street1 and Street2 columns. Also we switched Street1 and Street2 columns to match again as res2. Combined res1 with res2 together as our final merged result, res_inter.

Next we performed the similer process for pluto data. We loaded pluto data from `/home/vis/cr173/Sta523/data/nyc/pluto/pluto.Rdata`. And then we filterd `nyc7.2` by Address.Type == "ADDRESS" and extrated Incident.zip, Incident.address and borough columns from it as nyc_pluto. Then we merged nyc_pluto with pluto by matching "Address". Now we have two products, res_pluto and res_inter. We combined them together as "data" to plot later.



##Visualization##
After data cleaning, our final data consists of 628,250 observations with the corresponding longitude, latitude, and borough of the reported incident. First and foremost, we wanted to make sure that wercker would recognize our column variables `long` and `lat`, which, respectively, stand for `longitude` and `latitude`. 
`names(data) = c("Borough","long","lat")`    

We then took a random sample of the data. `data1 <- sample_frac(data, 0.1)` 
We conducted this random sampling based on two reasons. One, we believed that more data did not necessarily translate into better results as the location of the incident is also a random variable. That is, the location of the incident, may be within a borough or may be located on a borderline between two boroughs. This reasoning leads us to our second reasoning, where less data could exponentially reduce the running time of the svm model we utilized. Upon various levels of sensitivity analysis, we concluded not even though it may take 10 minutes to analyze 5% of the data, the increment in time does not necessarily vary linearly with the increment in percentage. That is, although it may only take 10 minutes to predict and plot 5% of the data, that does not mean that it will take 100 minutes to analyze 50% of the data. Thus, we believed that, in order to optimize performance and efficiency, we needed to take a random sample of the cleaned data. Via sensitivity analysis, we concluded that a 10% sample of the total data, performed the best and was the most efficient. 

Also, we decided that generating "fake data" was crucial in the performance of our svm model. This was based on the reasoning that fake data waas needed to help svm differentiate between points that are actually in one of the boroughs in New York and points that either, 1) belonged to the various bodies of water surrounding New York City or 2) belonged to non-New York regions, such as New Jersey. Thus, through pulling off coordinates from the GoogleMap and through usage of the `runif` command to generate larger areas with non-NYC coordinates, we were able to generate approximately 48000 non-NYC coordinates. We then combined the 10% of the cleaned data with all of the non-NYC data to build `data2`. 
`data2 <- rbind(data1,sample)` 

We converted the `long` and `lat` in `data2` to a numeric variable and necessarily dropped any NAs that came up via coercion. Finally, for svm to run, we converted `data2$Borough` to be a factor variable with six levels--Bronx, Brooklyn, Manhattan, Queens, Staten Island, and notNYC. 

We implemented a svm model `l<-svm(Borough ~ ., data2, kernel = "radial")`. In constructing the raster rectangle, we concluded that the automation of the contruction was necessary as we our data was a 10% sample of the original cleaned data. That is, our coordinates can differ according to the random sample r takes during each code run. Hence, we first specified a `dims` variable--namely, dimension--which takes the square root of the number of rows in `data2`, adds one to it, and throws out the decimals. Then we also specified a `boundary` variable which gives us the maximum and minimum of the `long` and `lat` variables in `data2`. As a final step, we set up the raster rectangle. We then proceed to predicting and plotting the points on our raster rectangle.

